---
title: "Homework 3"
author: "Ziyang (Bob) Ding"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:  html_document
latex_engine: xelatex
mainfont: "Times New Roman"
---

<style type="text/css">

h1.title {
  font-size: 38px;
  color: DarkRed;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkRed;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
  text-align: center;
}
</style>


## Problem 1

**For a sample $\left(Y_{1}, \ldots, Y_{n}\right)$ from $g,$ define weights**

$$
\begin{aligned}
w_{i}=\frac{\pi\left(Y_{i}\right) / g\left(Y_{i}\right)}{\sum_{j=1}^{n} \pi\left(Y_{j}\right) / g\left(Y_{j}\right)}
\end{aligned}
$$

**The sampling importance-resampling (SIR) algorithm (Rubin 1987 ) proceeds as follows:For $j=1, \ldots, m,$ take $X_{j}=Y_{i}$ with probability $w_{i}$**

**(a) Show that the SIR algorithm produces a sample from $f$ such that the empirical average Show that the sin angontitivity equivalent to the importance sampling estimator based on $\left(Y_{1}, \ldots, Y_{n}\right) .$**

**(b) What if the algorithm is modified to sample without replacement?**

&nbsp;

**Proof**

* (a) 

First, assume that we are doing replacements. In this way
    $$
        X_j \sim \{Y_1, Y_2 \dots Y_n\} \quad \mathbb{P}(X_j= Y_i) = w_i
    $$
    Therefore, the pdf $X_j$ has support on $\{Y_1, Y_2 \dots Y_n\}$. Then, because we know that
    $$
    \sum w_{i}= \sum \frac{\pi\left(Y_{i}\right) / g\left(Y_{i}\right)}{\sum_{j=1}^{n} \pi\left(Y_{j}\right) / g\left(Y_{j}\right)} = 1
    $$
    This let us know that when $m \to \infty$, we have that
    $$
    \begin{aligned}
    \frac{1}{m} \sum_{j=1}^{m} h\left(x_{j}\right) \rightarrow \mathbb{E}(h(x))&=\sum_{i=1}^{n} h\left(Y_{i}\right) \cdot W_{i} =\sum_{i=1}^{n} h\left(Y_{i}\right) \cdot \frac{\pi\left(Y_{i}\right) / g\left(Y_{i}\right)}{\sum \pi\left(Y_{i}\right) / g\left(Y_{i}\right)}\\
    &=\frac{\sum_{i=1}^{n} h\left(Y_{i}\right) \cdot \pi\left(Y_{i}\right) / g\left(Y_{i}\right)}{\sum_{i=1}^{n} \pi\left(Y_{i}\right) / g\left(Y_{i}\right)}=\frac{\sum_{i=1}^{n} h\left(Y_{i}\right) \cdot \omega_{i}^{'}}{\sum \omega_{i}^{'}}
    \end{aligned}
    $$
    We can observe that this is a normalized Importance Sampler. And
    $$
    \frac{\sum_{i=1}^{n} h\left(Y_{i}\right) \cdot w_{i}^{\prime}}{\sum w_{i}^{\prime}} \rightarrow \mathbb{E}(h(Y))
    $$
    Therefore, this is an asymptotically unbiased (consistent) estimator, which equivalent to the IS estimator.


&nbsp;
&nbsp;
&nbsp;

* (b) 

If sample without replacement, then $x$ will not follow a discrete distribution taking values $\left\{Y_{1}, Y_{2}, \ldots Y_{n}\right\}$ with probability $\left\{w_{1}, w_{2}, \cdots, w_{n}\right\}$ Instead, $X$ will have the same distribution as $Y,$ i.e. $X \sim g(X)$ Then $\frac{1}{m} \sum_{j=1}^{m} h\left(x_{j}\right)$ will be an estimate of $h(x)$ under $x \sim g(x)$.

```{r}
n <- 100000; m <- 100000
# SIR with replacement
Y <- runif(n, 0, 1)
pi <- dbeta(Y,2,5)
g <- dunif(Y,0,1)
W <- (pi/g)/(sum(pi/g))
X <- sample(Y, m, replace = TRUE, prob = W)
mean(X<0.5)
```

```{r}
# SIR without replacement
Y <- runif(n, 0, 1)
pi <- dbeta(Y,2,5)
g <- dunif(Y,0,1)
W <- (pi/g)/(sum(pi/g))
X <- sample(Y, m, replace = FALSE, prob = W)
mean(X<0.5)
```

As we can see the difference between replacement and not no replacement.


&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

## Problem 2

**Estimate the mean and variance of a $N(0,1)$ target distribution using importance sampling with instrumental distribution $t_{\nu}(0,1) .$ Plot your estimates as a function of sample size. Compare results for several values of $\nu$**

**Now repeat with $t_{\nu}$ as the target, and $N(0,1)$ as the approximating distribution. What do you observe? Relate your observations to the theoretical properties discussed in class.**

&nbsp;

**Proof**


```{r}
set.seed(1)
ImpSample <- function(s, nu){
x <- rt(s, nu)
w <- dnorm(x, 0, 1) / dt(x, nu)
mean <- mean(w * x)
var <- mean(w * x^2) - mean^2
return(c(mean, var))
}
RunAll <- function(nu1, nu2){
n_sample <- seq(100, 10000, 10)
vec_mean1 <- vec_var1 <- rep(NA, length(n_sample))
for(i in 1:length(n_sample)){
output <- ImpSample(n_sample[i], nu1)
vec_mean1[i] <- output[1]
vec_var1[i] <- output[2]
}
vec_mean2 <- vec_var2 <- rep(NA, length(n_sample))
for(i in 1:length(n_sample)){
output <- ImpSample(n_sample[i], nu2)
vec_mean2[i] <- output[1]
vec_var2[i] <- output[2]
}
par(mfrow=c(2,2))
plot(n_sample, vec_mean1, type = "l", main = paste("Mean vs Sample Size for nu = ",nu1))
plot(n_sample, vec_var1, type = "l", main = paste("Variance vs Sample Size for nu = ",nu1))
plot(n_sample, vec_mean2, type = "l", main = paste("Mean vs Sample Size for nu = ",nu2))
plot(n_sample, vec_var2, type = "l", main = paste("Variance vs Sample Size for nu = ",nu2))
}
# Run for different nu
RunAll(1, 5)
RunAll(10, 20)
```

**Similarity**

Noticablly, because $t_{\nu}$ distribution and normal distribution have similar density, we see that no matter what $\nu$ is, the sampling shows that an increasing sample size will help convergence of mean and variance to the correct value.


**Differnce**

Well, as $\nu$ increases, the variance of the estimators does not seem to vary much, so it's not sensitive to value of $\nu$. I think when $\nu$ become too big will cause a problem. But within a given range, the importance sampling should be fine. Sensitivity is not very high.

```{r}
set.seed(1)
ImpSample <- function(s, nu){
x <- rnorm(s, 0, 1)
w <- dt(x, nu) / dnorm(x, 0, 1)
mean <- sum(w * x) / sum(w)
var <- sum(w * x^2) / sum(w) - mean^2
return(c(mean, var))
}
RunAll <- function(nu1, nu2){
n_sample <- seq(100, 10000, 10)
vec_mean1 <- vec_var1 <- rep(NA, length(n_sample))
for(i in 1:length(n_sample)){
output <- ImpSample(n_sample[i], nu1)
vec_mean1[i] <- output[1]
vec_var1[i] <- output[2]
}
vec_mean2 <- vec_var2 <- rep(NA, length(n_sample))
for(i in 1:length(n_sample)){
output <- ImpSample(n_sample[i], nu2)
vec_mean2[i] <- output[1]
vec_var2[i] <- output[2]
}
par(mfrow=c(2,2))
plot(n_sample, vec_mean1, type = "l", main = paste("Mean vs Sample Size for nu = ",nu1))
plot(n_sample, vec_var1, type = "l", main = paste("Variance vs Sample Size for nu = ",nu1))
plot(n_sample, vec_mean2, type = "l", main = paste("Mean vs Sample Size for nu = ",nu2))
plot(n_sample, vec_var2, type = "l", main = paste("Variance vs Sample Size for nu = ",nu2))
}
# Run for different nu
RunAll(1, 5)
RunAll(10, 20)
```

**Similarity**

No matter what $\nu$ is, we see that the sampling doesn't bring up the convergence of mean and variance of target distribution. This might be that no matter $\nu$ is, t-distribution have heavier tail. Therefore, the sampling distribution is less concentrated than the target distribution. If we could (assume we could) sample from cauchy distribution, this would be even worse. Therefore, heavier tail distribution is not a good choice for importance sampling.

**Difference**

This time, we can see that as $\nu$ increases, the variances of estimators reduces. This is because as $\nu$ increases, the tail got lower and approximatly converge in distribution to a normal distribution. In this way, sampling procedure is more and more concentrated, and importance is more heavily sampled, which reduces variance of target estimator.

&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;




## Problem 3

**Show that if $X, Z \stackrel{\text { ind }}{\sim} U[0,1]$ and $Y=1 / Z,$ then $\operatorname{var}(X / Y)<\infty .$ Compare this with the variance obtained by the delta method. How do you account for the difference?**



&nbsp;

**Proof**

* Delta method:

Detta Method: since $\sigma_{XZ}=0, \quad$ use lst order approximation:
$$
\operatorname{var}\left(\frac{X}{Y}\right)=\operatorname{var}(XZ)=\sigma_{X}^{2} \cdot M_{X}^{2}+\sigma_{Z}^{2} \cdot \mu_{Z}^{2}+2 \cdot 0=\frac{1}{12} \cdot\left(\frac{1}{2}\right)^{2}+\frac{1}{12} \cdot\left(\frac{1}{2}\right)^{2}=\frac{1}{24}
$$

* This method:

$\quad \mathbb{E}(X)=\frac{1}{2}, \quad \operatorname{var}(X)=\operatorname{var}(Z)=\frac{1}{12} \cdot \mathbb{E}\left(X^{2}\right)=\mathbb{E}\left(Z^{2}\right)=\frac{1}{12}+\left(\frac{1}{2}\right)^{2}=\frac{1}{3}$

$$
\begin{align*}
\begin{aligned}
\operatorname{var}\left(\frac{X}{Y}\right) &=\operatorname{var}(XZ)=\mathbb{E}\left((XZ)^{2}\right)-\mathbb{E}(XZ)^{2}=\mathbb{E}\left(X^{2}\right)-\mathbb{E}(X)^{2} \cdot\left[E\left(Z^{2}\right)\right]\\
&=\frac{1}{3} \cdot \frac{1}{3}-\left(\frac{1}{2}\right)^{4}=\frac{1}{9}-\frac{1}{16}=\frac{7}{144}
\end{aligned}
\end{align*}
$$

note that $\frac{7}{144}>\frac{1}{24}$. If we could increase the order to maybe 3,5$\dots$, it delta would be even better. 
  
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;



## Problem 4

**Fit a logistic regression model to the O-ring failure dataset (RC Table 1.1) using importance sampling, as follows:**

**(a) Choose a prior distribution and derive the Newton-Raphson updates needed to find the posterior mode $\hat{\theta}$**

**(b) Implement the Newton-Raphson algorithm and obtain $\hat{\theta} .$ Justify any choices made. How confident are you that you have obtained the correct value of $\hat{\theta} ?$**

**(c) Perform importance sampling using a normal approximation to the posterior centered at your value of $\hat{\theta} .$ Use this to obtain posterior means, variances, and credible intervals for the parameters. Justify any choices made.**

**(d) Examine diagnostics for your importance sampling and report your conclusions.**

**(e) Replace the normal approximation with at $t_{3}$ distribution and repeat your importance sampling analysis. Do you observe any differences?**



&nbsp;

**Proof**

* (a)

$$
\begin{aligned}
    L(p) &\propto \prod_{i=1}^n p^{Y_i}(1-p)^{1-Y_i}\\
    P(p | y) &\propto P_{0}(p) \cdot f(y | p) \propto 1 \cdot f(y | p)\\
\end{aligned}
$$
$$
\begin{aligned}
-L L(Q) &=-\sum\left[Y_{i} \log \left(g\left(\theta^{\top} X_{i}\right)\right)+\left(1-Y_{i}\right) \cdot \log \left(1-g\left(\theta^{\top} X_{i}\right)\right)\right] \\
\frac{\partial(L L(\theta))}{\partial \theta_{j}} &=-\frac{n}{2}\left[Y_{i} \frac{\partial}{\partial \theta_{i}} \log \left(g\left(\theta^{\top} X_{i}\right)\right)+\left(1-Y_{i}\right) \cdot \frac{\partial}{\partial \theta_{i}} \log \left(1-g\left(\theta^{\top} X_{i}\right)\right)\right] \\
&=-\frac{n}{2}\left[Y_{i} \frac{\partial g\left(\theta^{\top} X_{i}\right)}{g\left(\theta^{\top} X_{i}\right)}+\left(1-Y_{i}\right) \frac{\partial^{2}\left(1-g\left(\theta^{\top} X_{i}\right)\right)}{1-g\left(\theta^{\top} X_{i}\right)}\right] \\
g\left(\theta^{\top} X\right) &=\frac{e^{\theta X}}{1+e^{\theta X}}=\frac{1}{1+e^{-\theta X}} \frac{\partial}{\partial \theta_{j}} g\left(\theta^{\top} X\right)\\ &=\frac{1}{\left(1+e^{-\theta(X)}\right)^{2}}(-1) \cdot e^{-\theta^{\top} X} \cdot\left(-X_{j}\right)\\
&=\frac{e^{-\theta X}}{1+e^{\theta(X}} \cdot \frac{1}{1+e^{-\theta^{\top} X}} \cdot X_{j}=g\left(\theta^{\top} X\right) \cdot X_{j}
\end{aligned}
$$

$$
\begin{aligned}
\frac{\partial(-L L(\theta))}{\partial \theta_{j}} &=-\sum_{i}^{n}\left[Y_{i}\left(1-g\left(\theta^{\top} X_{i}\right)\right) \cdot X_{i j}+\left(Y_{i}-1\right) g\left(\theta^{\top} X_{i}\right) X_{i j}\right] \\
&=-\sum_{i}^{n}\left[Y_{i} X_{i j}-g\left(\theta^{\top} X_{i}\right) X_{i j}\right] \\
&=\sum_{i}^{n}\left(g\left(\theta^{\top} X_{i}\right)-Y_{i}\right) X_{i j}
\end{aligned} 
$$

$$
\begin{aligned}
    H&=\left[\begin{array}{ll}
{\frac{\partial^{2}}{\partial a^{2}}(-L L(\theta))} & {\frac{\partial^{2}}{\partial \theta \partial \theta_{1}}(-L L(\theta))} \\
{\frac{\partial^{2}}{\partial \theta_{1} \partial \theta_{2}}(-L L(\theta))} & {\frac{\partial^{2}}{\partial \theta^{2}}(-L L(\underline{\theta}))}
\end{array}\right]\\
&=\left[\begin{array}{ll}
{\frac{n}{2} g\left(\theta^{\top} X_{i}\right)\left(-g\left(\theta^{\top} X_{i}\right)\right) \cdot l^{2}} & {\frac{n}{2} g\left(\theta^{\top} X_{i}\right)\left(+g\left(\theta^{\top} X_{i}\right)\right) \cdot X_{i 2}} \\
{\frac{n}{2} g\left(\theta^{\top} X_{i}\right)\left(1-g\left(\theta^{\top} X_{i}\right)\right) \cdot X_{i 2}} & {\sum_{i=1}^{n} g\left(\theta^{\top} X_{i}\right)\left(1-g\left(\theta^{\top} X_{i}\right)\right) X_{i}^{2}}
\end{array}\right]
\end{aligned}
$$

&nbsp;
&nbsp;
&nbsp;

* (b)

```{r}
sigmoid <- function(theta, xi){
theta_xi <- theta[1] + theta[2] * xi
return(1/(1+exp(-theta_xi)))
}
gradient <- function(theta, x, y){
g1 <- g2 <- 0
for(i in 1:length(y)){
g1 <- g1 + (sigmoid(theta, x[i]) - y[i])
g2 <- g2 + (sigmoid(theta, x[i]) - y[i]) * x[i]
}
return(c(g1, g2))
}
hessian <- function(theta, x, y){
h <- matrix(rep(0,4), nrow = 2, ncol = 2)
for(i in 1:length(y)){
h[1,1] <- h[1,1] + sigmoid(theta, x[i])*(1-sigmoid(theta, x[i]))
h[1,2] <- h[1,2] + sigmoid(theta, x[i])*(1-sigmoid(theta, x[i])) * x[i]
h[2,1] <- h[1,2]
h[2,2] <- h[2,2] + sigmoid(theta, x[i])*(1-sigmoid(theta, x[i])) * (x[i]^2)
}
return(h)
}
## data
x <- c(53,57,58,63,66,67,67,67,68,69,70,70,70,70,72,73,75,75,76,76,78,79,81)
y <- c(1,1,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0)
n_iter <- 10000
THETA <- matrix(rep(NA, n_iter*2), ncol=2)
THETA[1,] <- c(0,0)
for(s in 2:n_iter){
temp <- THETA[s-1,]
g <- gradient(temp, x, y)
h <- hessian(temp, x, y)
q <- qr.Q(qr(h))
r <- qr.R(qr(h))
Rtheta <- r %*% temp - t(q) %*% g
THETA[s,] <- backsolve(r, Rtheta) # backsolve(R, b) for x: Rx = b
}
par(mfrow=c(1,2))
plot(1:n_iter, THETA[,1], type = "l")
plot(1:n_iter, THETA[,2], type = "l")
(saveTHETA <- THETA[n_iter,])  
lm.fit <- glm(y ~ x, family=binomial(link="logit"))
coef(lm.fit)
```


&nbsp;
&nbsp;
&nbsp;

* (c)

```{r}
set.seed(123)
library(mvtnorm)

```

OneSample_IS_norm <- function(theta_start, covmatrix){
  theta <- rmvnorm(1, theta_start, covmatrix)
  pi <- 1
    for(i in 1:length(y)){
    p <- sigmoid(theta, x[i])
    pi <- pi * p^y[i] * (1-p)^(1-y[i])
    }
  w <- pi / dmvnorm(theta, theta_start, covmatrix)
  return(c(theta, w))
}

theta_start <- saveTHETA
covmatrix <- matrix(c(1,0,0,1), ncol=2)
n_sample <- seq(100, 10000, 20)
MEAN <- VAR <- matrix(rep(NA, length(n_sample)*2), ncol=2)

for (m in 1:length(n_sample)){
  n_iter <- n_sample[m]
  THETA1 <- THETA2 <- W <- rep(NA,n_iter)
  for (s in 1:n_iter){
    output <- OneSample_IS_norm(theta_start, covmatrix)
    THETA1[s] <- output[1]
    THETA2[s] <- output[2]
    W[s] <- output[3]
    mean1 <- sum(W * THETA1) / sum(W)
    var1 <- sum(W * THETA1^2) / sum(W) - mean1^2
    mean2 <- sum(W * THETA2) / sum(W)
    var2 <- sum(W * THETA2^2) / sum(W) - mean2^2
  }
  MEAN[m,1] <- mean1
  MEAN[m,2] <- mean2
  VAR[m,1] <- var1
  VAR[m,2] <- var2
}

# Posterior Means
apply(MEAN, 2, mean)
# Variances
apply(VAR, 2, mean)
# 95% Credible Intervals
apply(MEAN, 2, function(x) {quantile(x, c(0.025, 0.975))})

"""

## [1] 15.1005576 -0.2341132
## [1] 2.5496869828 0.0006141231
## [,1] [,2]
## 2.5% 12.83670 -0.2668712
## 97.5% 17.27207 -0.2006513

&nbsp;
&nbsp;
&nbsp;

* (d)



par(mfrow=c(2,2))
plot(n_sample, MEAN[,1], type = "l", main="Mean for Theta1 (Intercpet)")
plot(n_sample, VAR[,1], type = "l", main="Variance for Theta1 (Intercpet)")
plot(n_sample, MEAN[,2], type = "l", main="Mean for Theta2 (Temperature)")
plot(n_sample, VAR[,2], type = "l", main="Variance for Theta2 (Temperature)")



&nbsp;
&nbsp;
&nbsp;

* (e)

```{r}
set.seed(123)
library(mnormt)
```

OneSample_IS_t <- function(theta_start, S){
  theta <- rmt(1, theta_start, S)
  pi <- 1
  for(i in 1:length(y)){
    p <- sigmoid(theta, x[i])
    pi <- pi * p^y[i] * (1-p)^(1-y[i])
  }
w <- pi / dmt(theta, theta_start, S)
return(c(theta, w))
}

theta_start <- saveTHETA
S <- matrix(c(1,0,0,1), ncol=2)
n_sample <- seq(100, 10000, 20)
MEAN <- VAR <- matrix(rep(NA, length(n_sample)*2), ncol=2)

for (m in 1:length(n_sample)){
  n_iter <- n_sample[m]
  THETA1 <- THETA2 <- W <- rep(NA,n_iter)

  for (s in 1:n_iter){
    output <- OneSample_IS_t(theta_start, S)
    THETA1[s] <- output[1]
    THETA2[s] <- output[2]
    W[s] <- output[3]
    mean1 <- sum(W * THETA1) / sum(W)
    var1 <- sum(W * THETA1^2) / sum(W) - mean1^2
    mean2 <- sum(W * THETA2) / sum(W)
    var2 <- sum(W * THETA2^2) / sum(W) - mean2^2
  }
    
  MEAN[m,1] <- mean1
  MEAN[m,2] <- mean2
  VAR[m,1] <- var1
  VAR[m,2] <- var2
}


# Posterior Means
apply(MEAN, 2, mean)
# Variances
apply(VAR, 2, mean)
# 95% Credible Intervals
apply(MEAN, 2, function(x) {quantile(x, c(0.025, 0.975))})



par(mfrow=c(2,2))


plot(n_sample, MEAN[,1], type = "l", main="Mean for Theta1 (Intercpet)")
plot(n_sample, VAR[,1], type = "l", main="Variance for Theta1 (Intercpet)")
plot(n_sample, MEAN[,2], type = "l", main="Mean for Theta2 (Temperature)")
plot(n_sample, VAR[,2], type = "l", main="Variance for Theta2 (Temperature)")





&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;






























